{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12519500,"sourceType":"datasetVersion","datasetId":7902587}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7f646aa9","cell_type":"code","source":"# Cell 1: Enhanced imports and setup\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# For holiday calendar\ntry:\n    from pandas.tseries.holiday import USFederalHolidayCalendar\n    HOLIDAY_CALENDAR_AVAILABLE = True\nexcept ImportError:\n    HOLIDAY_CALENDAR_AVAILABLE = False\n    print(\"Warning: Holiday calendar not available, will skip holiday features\")\n\n# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/hopeless/train_kanafren.csv')\ntest_df = pd.read_csv('/kaggle/input/hopeless/test_kanafren.csv')\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\nprint(f\"Train columns: {len(train_df.columns)}\")\nprint(f\"Test columns: {len(test_df.columns)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:39:07.109292Z","iopub.execute_input":"2025-07-20T14:39:07.109605Z","iopub.status.idle":"2025-07-20T14:40:36.352046Z","shell.execute_reply.started":"2025-07-20T14:39:07.109581Z","shell.execute_reply":"2025-07-20T14:40:36.351077Z"}},"outputs":[{"name":"stdout","text":"Train shape: (770164, 391)\nTest shape: (369301, 390)\nTrain columns: 391\nTest columns: 390\n","output_type":"stream"}],"execution_count":1},{"id":"cbc3b712","cell_type":"code","source":"# Cell 2: Strategic pre-imputation\ndef strategic_pre_imputation(df):\n    \"\"\"Strategic pre-imputation for features critical to feature engineering\"\"\"\n    df_imputed = df.copy()\n    \n    # 1. Interest scores (f1-f12) - median by user activity level\n    interest_cols = [f'f{i}' for i in range(1, 13) if f'f{i}' in df.columns]\n    \n    # Create user activity tier for grouping\n    if 'id2' in df.columns:\n        user_impression_counts = df.groupby('id2').size()\n        df_imputed['user_activity_tier'] = pd.cut(\n            df_imputed['id2'].map(user_impression_counts),\n            bins=[0, 1, 5, 20, float('inf')],\n            labels=['low', 'medium', 'high', 'very_high']\n        )\n        \n        # Median imputation by activity tier\n        for col in interest_cols:\n            df_imputed[col] = df_imputed.groupby('user_activity_tier')[col].transform(\n                lambda x: x.fillna(x.median())\n            )\n    else:\n        # Fallback to global median\n        for col in interest_cols:\n            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n    \n    # 2. Loyalty features - 0 for numerical (no participation), mode for categorical\n    loyalty_numerical = ['f43', 'f46', 'f47', 'f51', 'f58'] # Miles, tenure, etc.\n    loyalty_categorical = ['f42', 'f54', 'f55'] # Membership level, status\n    \n    for col in loyalty_numerical:\n        if col in df_imputed.columns:\n            df_imputed[col] = df_imputed[col].fillna(0)\n    \n    for col in loyalty_categorical:\n        if col in df_imputed.columns:\n            mode_val = df_imputed[col].mode().iloc[0] if not df_imputed[col].mode().empty else 'Unknown'\n            df_imputed[col] = df_imputed[col].fillna(mode_val)\n    \n    # 3. Spending patterns (f152-f198) - 0 (no spending)\n    spending_cols = [f'f{i}' for i in range(152, 199) if f'f{i}' in df.columns]\n    for col in spending_cols:\n        df_imputed[col] = df_imputed[col].fillna(0)\n    \n    # 4. CTR features (f104-f113, f310-f314) - global median for established users, 0 for new\n    ctr_cols = [f'f{i}' for i in range(104, 114)] + [f'f{i}' for i in range(310, 315)]\n    ctr_cols = [col for col in ctr_cols if col in df_imputed.columns]\n    \n    for col in ctr_cols:\n        global_median = df_imputed[col].median()\n        df_imputed[col] = df_imputed[col].fillna(global_median if global_median > 0 else 0)\n    \n    # 5. Transaction frequency features (f174-f198) - 0 (no transactions)\n    transaction_cols = [f'f{i}' for i in range(174, 199) if f'f{i}' in df.columns]\n    for col in transaction_cols:\n        df_imputed[col] = df_imputed[col].fillna(0)\n    \n    # Clean up temporary column\n    if 'user_activity_tier' in df_imputed.columns:\n        df_imputed = df_imputed.drop(columns=['user_activity_tier'])\n    \n    return df_imputed\n\n# Apply strategic pre-imputation\nprint(\"Applying strategic pre-imputation...\")\ntrain_df = strategic_pre_imputation(train_df)\ntest_df = strategic_pre_imputation(test_df)\nprint(f\"Pre-imputation completed. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:36.353635Z","iopub.execute_input":"2025-07-20T14:40:36.353954Z","iopub.status.idle":"2025-07-20T14:40:42.462090Z","shell.execute_reply.started":"2025-07-20T14:40:36.353931Z","shell.execute_reply":"2025-07-20T14:40:42.461250Z"}},"outputs":[{"name":"stdout","text":"Applying strategic pre-imputation...\nPre-imputation completed. Train shape: (770164, 391), Test shape: (369301, 390)\n","output_type":"stream"}],"execution_count":2},{"id":"678f9d61","cell_type":"code","source":"# Rename columns to ensure consistency between train and test\nif 'f374_x' in train_df.columns:\n    train_df.rename(columns={'f374_x': 'f374'}, inplace=True)\nif 'f374_x' in test_df.columns:\n    test_df.rename(columns={'f374_x': 'f374'}, inplace=True)\n\n# Check available offer metadata columns\noffer_metadata_cols = ['id3', 'f374', 'f376', 'id8', 'id12', 'id13']\nexisting_cols = [col for col in offer_metadata_cols if col in train_df.columns]\nprint(\"Available offer metadata columns:\", existing_cols)\n\n# Display sample data\nprint(\"\\nSample offer metadata (train):\")\nprint(train_df[existing_cols].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:42.463092Z","iopub.execute_input":"2025-07-20T14:40:42.463387Z","iopub.status.idle":"2025-07-20T14:40:42.501492Z","shell.execute_reply.started":"2025-07-20T14:40:42.463359Z","shell.execute_reply":"2025-07-20T14:40:42.500506Z"}},"outputs":[{"name":"stdout","text":"Available offer metadata columns: ['id3', 'f374', 'f376', 'id8', 'id12', 'id13']\n\nSample offer metadata (train):\n         id3                                               f374  f376  \\\n0  189706075  Radio, Television, And Consumer Electronics St...   2.0   \n1      89227                                      Liquor Stores   NaN   \n2      35046                                       Beauty Shops  10.0   \n3    6275451                             Family Clothing Stores  10.0   \n4      78053                   Miscellaneous Retail Stores, Nec   8.0   \n\n          id8        id12                 id13  \n0  57310000.0  2023-11-01  2023-11-30 23:59:59  \n1  59210000.0  2023-11-01  2024-04-30 23:59:59  \n2  72310000.0  2023-11-01  2023-11-30 23:59:59  \n3  56510500.0  2023-11-01  2023-11-30 23:59:59  \n4  59991300.0  2023-11-01  2023-11-30 23:59:59  \n","output_type":"stream"}],"execution_count":3},{"id":"d45ee068","cell_type":"code","source":"# Cell 3: Prepare datetime columns and sort data (CRITICAL FIX)\ndef prepare_temporal_data(df):\n    \"\"\"Prepare temporal data and sort by user and time to prevent data leakage\"\"\"\n    df_prepared = df.copy()\n    \n    # Convert datetime columns\n    df_prepared['impression_time'] = pd.to_datetime(df_prepared['id4'])\n    df_prepared['click_time'] = pd.to_datetime(df_prepared['id7'], errors='coerce')\n    \n    # CRITICAL: Sort by user and time to enable proper temporal aggregations\n    df_prepared = df_prepared.sort_values(['id2', 'impression_time']).reset_index(drop=True)\n    \n    # Create click indicator early\n    df_prepared['has_clicked'] = df_prepared['click_time'].notna().astype(int)\n    \n    return df_prepared\n\n# Apply temporal preparation\nprint(\"Preparing temporal data and sorting...\")\ntrain_df = prepare_temporal_data(train_df)\ntest_df = prepare_temporal_data(test_df)\nprint(f\"Temporal preparation completed. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:42.503120Z","iopub.execute_input":"2025-07-20T14:40:42.503413Z","iopub.status.idle":"2025-07-20T14:40:52.562614Z","shell.execute_reply.started":"2025-07-20T14:40:42.503391Z","shell.execute_reply":"2025-07-20T14:40:52.561764Z"}},"outputs":[{"name":"stdout","text":"Preparing temporal data and sorting...\nTemporal preparation completed. Train shape: (770164, 394), Test shape: (369301, 393)\n","output_type":"stream"}],"execution_count":4},{"id":"81bcb31f","cell_type":"code","source":"# Cell 4: Data Analysis and Dynamic Threshold Calculation\ndef calculate_optimal_time_threshold(df, method='quantile', train_ratio=0.8, min_val_samples=1000):\n    \"\"\"Calculate optimal time threshold for train/validation split\"\"\"\n    \n    if 'impression_time' not in df.columns:\n        raise ValueError(\"impression_time column not found\")\n    \n    total_samples = len(df)\n    \n    print(\"Data time range analysis:\")\n    print(f\"Earliest impression: {df['impression_time'].min()}\")\n    print(f\"Latest impression: {df['impression_time'].max()}\")\n    print(f\"Total date span: {(df['impression_time'].max() - df['impression_time'].min()).days} days\")\n    \n    # Check data distribution by date\n    daily_counts = df.groupby(df['impression_time'].dt.date).size()\n    print(f\"\\nDaily data distribution:\")\n    print(daily_counts)\n    \n    if method == 'quantile':\n        # Use quantile-based approach\n        threshold = df['impression_time'].quantile(train_ratio)\n        \n    elif method == 'sample_based':\n        # Ensure minimum validation samples\n        sorted_dates = df['impression_time'].sort_values()\n        val_samples_needed = max(min_val_samples, int(total_samples * (1 - train_ratio)))\n        split_index = total_samples - val_samples_needed\n        threshold = sorted_dates.iloc[split_index]\n        \n    elif method == 'daily_boundary':\n        # Split on day boundaries to avoid splitting user sessions\n        daily_counts_cumsum = daily_counts.cumsum()\n        target_train_samples = int(total_samples * train_ratio)\n        \n        # Find the day where cumulative count exceeds target\n        split_day = daily_counts_cumsum[daily_counts_cumsum >= target_train_samples].index[0]\n        threshold = pd.Timestamp(split_day)\n        \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    # Validate the threshold\n    train_count = (df['impression_time'] < threshold).sum()\n    val_count = (df['impression_time'] >= threshold).sum()\n    \n    print(f\"\\nThreshold calculation results:\")\n    print(f\" Method: {method}\")\n    print(f\" Threshold: {threshold}\")\n    print(f\" Train samples: {train_count} ({train_count/total_samples:.1%})\")\n    print(f\" Validation samples: {val_count} ({val_count/total_samples:.1%})\")\n    \n    if val_count == 0:\n        print(\"WARNING: No validation data with this threshold!\")\n        return None\n        \n    return threshold\n\n# Calculate optimal threshold for your data\noptimal_threshold = calculate_optimal_time_threshold(\n    train_df, \n    method='quantile',  # Try 'sample_based' or 'daily_boundary' if needed\n    train_ratio=0.8\n)\n\nprint(f\"\\nOptimal threshold selected: {optimal_threshold}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:52.563571Z","iopub.execute_input":"2025-07-20T14:40:52.564421Z","iopub.status.idle":"2025-07-20T14:40:52.861264Z","shell.execute_reply.started":"2025-07-20T14:40:52.564396Z","shell.execute_reply":"2025-07-20T14:40:52.860351Z"}},"outputs":[{"name":"stdout","text":"Data time range analysis:\nEarliest impression: 2023-11-01 00:00:23.729000\nLatest impression: 2023-11-03 23:59:58.572000\nTotal date span: 2 days\n\nDaily data distribution:\nimpression_time\n2023-11-01    301698\n2023-11-02    246621\n2023-11-03    221845\ndtype: int64\n\nThreshold calculation results:\n Method: quantile\n Threshold: 2023-11-03 08:30:56.586400\n Train samples: 616131 (80.0%)\n Validation samples: 154033 (20.0%)\n\nOptimal threshold selected: 2023-11-03 08:30:56.586400\n","output_type":"stream"}],"execution_count":5},{"id":"abd0422e","cell_type":"markdown","source":"## Phase 1","metadata":{}},{"id":"4a15f469","cell_type":"code","source":"# Cell 5: Merchant Category Features\ndef create_merchant_category_features(df):\n    \"\"\"Create merchant category features from f374 column\"\"\"\n    df_features = df.copy()\n    \n    # One-hot encode merchant categories (limit to top categories to avoid high dimensionality)\n    top_categories = df['f374'].value_counts().head(20).index\n    df_features['f374_top'] = df_features['f374'].apply(lambda x: x if x in top_categories else 'Other')\n    \n    # Create dummy variables for top categories\n    category_dummies = pd.get_dummies(df_features['f374_top'], prefix='merchant_cat')\n    df_features = pd.concat([df_features, category_dummies], axis=1)\n    \n    # Category frequency encoding\n    category_counts = df['f374'].value_counts()\n    df_features['merchant_cat_frequency'] = df_features['f374'].map(category_counts)\n    \n    # Category rarity score (inverse frequency)\n    df_features['merchant_cat_rarity'] = 1 / df_features['merchant_cat_frequency']\n    \n    return df_features\n\n# Apply merchant category features\nprint(\"Creating merchant category features...\")\ntrain_df_features = create_merchant_category_features(train_df)\ntest_df_features = create_merchant_category_features(test_df)\nprint(f\"Train shape after merchant features: {train_df_features.shape}\")\nprint(f\"Test shape after merchant features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:52.862398Z","iopub.execute_input":"2025-07-20T14:40:52.862687Z","iopub.status.idle":"2025-07-20T14:40:57.639997Z","shell.execute_reply.started":"2025-07-20T14:40:52.862658Z","shell.execute_reply":"2025-07-20T14:40:57.638829Z"}},"outputs":[{"name":"stdout","text":"Creating merchant category features...\nTrain shape after merchant features: (770164, 418)\nTest shape after merchant features: (369301, 417)\n","output_type":"stream"}],"execution_count":6},{"id":"d883f8df","cell_type":"code","source":"# Cell 6: Enhanced Temporal Features\ndef create_enhanced_temporal_features(df):\n    \"\"\"Create enhanced temporal features with proper time-based logic\"\"\"\n    df_features = df.copy()\n    \n    # Basic temporal components\n    df_features['impression_hour'] = df_features['impression_time'].dt.hour\n    df_features['impression_day_of_week'] = df_features['impression_time'].dt.dayofweek\n    df_features['impression_day_of_month'] = df_features['impression_time'].dt.day\n    df_features['impression_month'] = df_features['impression_time'].dt.month\n    df_features['impression_quarter'] = df_features['impression_time'].dt.quarter\n    \n    # Time-based categories\n    df_features['is_weekend_impression'] = (df_features['impression_day_of_week'] >= 5).astype(int)\n    df_features['is_business_hours'] = (\n        (df_features['impression_hour'] >= 9) & (df_features['impression_hour'] <= 17)\n    ).astype(int)\n    \n    # Daypart interaction features\n    df_features['weekend_business_hours'] = (\n        df_features['is_weekend_impression'] & df_features['is_business_hours']\n    ).astype(int)\n    \n    # Time of day categories\n    df_features['time_of_day'] = pd.cut(\n        df_features['impression_hour'],\n        bins=[0, 6, 12, 18, 24],\n        labels=['night', 'morning', 'afternoon', 'evening'],\n        include_lowest=True\n    )\n    \n    # Cyclical encoding for temporal features\n    df_features['hour_sin'] = np.sin(2 * np.pi * df_features['impression_hour'] / 24)\n    df_features['hour_cos'] = np.cos(2 * np.pi * df_features['impression_hour'] / 24)\n    df_features['day_sin'] = np.sin(2 * np.pi * df_features['impression_day_of_week'] / 7)\n    df_features['day_cos'] = np.cos(2 * np.pi * df_features['impression_day_of_week'] / 7)\n    \n    # Time since last interaction (FIXED for data leakage)\n    df_features['time_since_last_impression'] = df_features.groupby('id2')['impression_time'].diff().dt.total_seconds()\n    df_features['time_since_last_impression'] = df_features['time_since_last_impression'].fillna(0)\n    \n    # Time since last click (only for users who have clicked before)\n    df_features['time_since_last_click'] = df_features.groupby('id2')['click_time'].diff().dt.total_seconds()\n    df_features['time_since_last_click'] = df_features['time_since_last_click'].fillna(np.inf)\n    \n    # Holiday proximity features\n    if HOLIDAY_CALENDAR_AVAILABLE:\n        try:\n            holiday_calendar = USFederalHolidayCalendar()\n            holidays = holiday_calendar.holidays(\n                start=df_features['impression_time'].min(), \n                end=df_features['impression_time'].max()\n            )\n            \n            def days_to_nearest_holiday(date):\n                if len(holidays) == 0:\n                    return 365 # Default if no holidays\n                return min([abs((date - h).days) for h in holidays])\n            \n            df_features['days_to_holiday'] = df_features['impression_time'].apply(days_to_nearest_holiday)\n            df_features['is_near_holiday'] = (df_features['days_to_holiday'] <= 7).astype(int)\n        except:\n            df_features['days_to_holiday'] = 365\n            df_features['is_near_holiday'] = 0\n    else:\n        df_features['days_to_holiday'] = 365\n        df_features['is_near_holiday'] = 0\n    \n    return df_features\n\n# Apply enhanced temporal features\nprint(\"Creating enhanced temporal features...\")\ntrain_df_features = create_enhanced_temporal_features(train_df_features)\ntest_df_features = create_enhanced_temporal_features(test_df_features)\nprint(f\"Train shape after temporal features: {train_df_features.shape}\")\nprint(f\"Test shape after temporal features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:57.640966Z","iopub.execute_input":"2025-07-20T14:40:57.641268Z","iopub.status.idle":"2025-07-20T14:41:05.492468Z","shell.execute_reply.started":"2025-07-20T14:40:57.641245Z","shell.execute_reply":"2025-07-20T14:41:05.491703Z"}},"outputs":[{"name":"stdout","text":"Creating enhanced temporal features...\nTrain shape after temporal features: (770164, 435)\nTest shape after temporal features: (369301, 434)\n","output_type":"stream"}],"execution_count":7},{"id":"cb640a73","cell_type":"code","source":"# Cell 7: Fixed User Session Features\ndef create_user_session_features_fixed(df):\n    \"\"\"Create user session features with proper temporal ordering to prevent data leakage\"\"\"\n    df_features = df.copy()\n    \n    # CRITICAL: Data must be sorted by user and time before cumulative operations\n    df_features = df_features.sort_values(['id2', 'impression_time']).reset_index(drop=True)\n    \n    # FIXED: Cumulative features (no future data leakage)\n    df_features['user_cumulative_impressions'] = df_features.groupby('id2').cumcount()\n    df_features['user_cumulative_clicks'] = df_features.groupby('id2')['has_clicked'].cumsum()\n    \n    # FIXED: Rolling CTR using proper approach for groupby + rolling\n    df_features = df_features.set_index('impression_time')\n    \n    # Rolling CTR (7-day window, no future data)\n    df_features['user_rolling_ctr_7d'] = (\n        df_features.groupby('id2')['has_clicked']\n        .rolling('7D', min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    \n    # Rolling CTR (3-day window)\n    df_features['user_rolling_ctr_3d'] = (\n        df_features.groupby('id2')['has_clicked']\n        .rolling('3D', min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    \n    # Rolling click counts (no future data)\n    df_features['clicks_last_3d'] = (\n        df_features.groupby('id2')['has_clicked']\n        .rolling('3D', min_periods=1)\n        .sum()\n        .reset_index(level=0, drop=True)\n    )\n    \n    # Reset index to get impression_time back as a column\n    df_features = df_features.reset_index()\n    \n    # User engagement velocity (change in CTR)\n    df_features['user_ctr_trend'] = (\n        df_features['user_rolling_ctr_3d'] - \n        df_features.groupby('id2')['user_rolling_ctr_3d'].shift(1)\n    ).fillna(0)\n    \n    # Session sequence features\n    df_features['offer_position_in_session'] = (\n        df_features.groupby(['id2', df_features['impression_time'].dt.date]).cumcount() + 1\n    )\n    \n    # User activity patterns\n    df_features['user_session_length'] = df_features.groupby(['id2', df_features['impression_time'].dt.date])['id3'].transform('count')\n    \n    # User consistency metrics\n    df_features['user_avg_session_length'] = (\n        df_features.groupby('id2')['user_session_length']\n        .expanding()\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    \n    return df_features\n\n# Apply fixed user session features\nprint(\"Creating fixed user session features...\")\ntrain_df_features = create_user_session_features_fixed(train_df_features)\ntest_df_features = create_user_session_features_fixed(test_df_features)\nprint(f\"Train shape after user session features: {train_df_features.shape}\")\nprint(f\"Test shape after user session features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:41:05.493439Z","iopub.execute_input":"2025-07-20T14:41:05.493725Z","iopub.status.idle":"2025-07-20T14:41:29.031495Z","shell.execute_reply.started":"2025-07-20T14:41:05.493696Z","shell.execute_reply":"2025-07-20T14:41:29.030556Z"}},"outputs":[{"name":"stdout","text":"Creating fixed user session features...\nTrain shape after user session features: (770164, 444)\nTest shape after user session features: (369301, 443)\n","output_type":"stream"}],"execution_count":8},{"id":"4866407c","cell_type":"code","source":"# Cell 8: Fixed Page Context Features\ndef create_page_context_features_fixed(df):\n    \"\"\"Create page context features with proper temporal ordering\"\"\"\n    df_features = df.copy()\n    \n    # Page type encoding\n    df_features['page_type'] = df_features['id6']\n    \n    # One-hot encode page types\n    page_dummies = pd.get_dummies(df_features['page_type'], prefix='page')\n    df_features = pd.concat([df_features, page_dummies], axis=1)\n    \n    # FIXED: Page performance with expanding window (no future data)\n    df_features = df_features.sort_values(['id6', 'impression_time']).reset_index(drop=True)\n    \n    # Expanding page statistics (only uses historical data)\n    df_features['page_cumulative_impressions'] = df_features.groupby('id6').cumcount() + 1\n    df_features['page_cumulative_clicks'] = df_features.groupby('id6')['has_clicked'].cumsum()\n    \n    # Page CTR using only historical data\n    df_features['page_historical_ctr'] = (\n        df_features['page_cumulative_clicks'] / df_features['page_cumulative_impressions']\n    )\n    \n    # Page engagement level based on historical performance\n    df_features['page_engagement_tier'] = pd.cut(\n        df_features['page_historical_ctr'],\n        bins=[0, 0.02, 0.05, 0.1, 1.0],\n        labels=[1, 2, 3, 4]\n    ).fillna(1).astype(int)\n    \n    # FIXED: User-page interaction patterns (expanding window)\n    df_features = df_features.sort_values(['id2', 'id6', 'impression_time']).reset_index(drop=True)\n    \n    df_features['user_page_cumulative_impressions'] = df_features.groupby(['id2', 'id6']).cumcount() + 1\n    df_features['user_page_cumulative_clicks'] = df_features.groupby(['id2', 'id6'])['has_clicked'].cumsum()\n    \n    # User-page historical CTR\n    df_features['user_page_historical_ctr'] = (\n        df_features['user_page_cumulative_clicks'] / df_features['user_page_cumulative_impressions']\n    )\n    \n    # Page preference score (user's historical performance vs page average)\n    df_features['user_page_preference'] = (\n        df_features['user_page_historical_ctr'] / (df_features['page_historical_ctr'] + 0.001)\n    )\n    \n    return df_features\n\n# Apply fixed page context features\nprint(\"Creating fixed page context features...\")\ntrain_df_features = create_page_context_features_fixed(train_df_features)\ntest_df_features = create_page_context_features_fixed(test_df_features)\nprint(f\"Train shape after page context features: {train_df_features.shape}\")\nprint(f\"Test shape after page context features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:41:29.032372Z","iopub.execute_input":"2025-07-20T14:41:29.032594Z","iopub.status.idle":"2025-07-20T14:41:51.866470Z","shell.execute_reply.started":"2025-07-20T14:41:29.032577Z","shell.execute_reply":"2025-07-20T14:41:51.865508Z"}},"outputs":[{"name":"stdout","text":"Creating fixed page context features...\nTrain shape after page context features: (770164, 453)\nTest shape after page context features: (369301, 452)\n","output_type":"stream"}],"execution_count":9},{"id":"c30f368c","cell_type":"markdown","source":"## Phase 2 - Advanced Features","metadata":{}},{"id":"f9a377f6","cell_type":"code","source":"# Cell 9: Enhanced Interest-Offer Alignment Features\ndef create_enhanced_interest_offer_alignment(df):\n    \"\"\"Enhanced interest-offer alignment with gap analysis and interactions\"\"\"\n    df_features = df.copy()\n    \n    # Define interest categories mapping\n    interest_mapping = {\n        'automotive': ['f1'],\n        'dining': ['f2', 'f8'],\n        'business': ['f3', 'f5', 'f6', 'f10'],\n        'hobby': ['f4', 'f12'],\n        'home': ['f7'],\n        'travel': ['f9'],\n        'electronics': ['f11']\n    }\n    \n    # Calculate category interest scores\n    for category, features in interest_mapping.items():\n        available_features = [f for f in features if f in df_features.columns]\n        if available_features:\n            df_features[f'interest_{category}_score'] = df_features[available_features].mean(axis=1)\n        else:\n            df_features[f'interest_{category}_score'] = 0\n    \n    # Overall interest metrics\n    interest_cols = [f'f{i}' for i in range(1, 13) if f'f{i}' in df_features.columns]\n    if interest_cols:\n        df_features['interest_total_score'] = df_features[interest_cols].sum(axis=1)\n        df_features['interest_diversity_score'] = (df_features[interest_cols] > 0).sum(axis=1)\n        df_features['interest_strength_ratio'] = df_features[interest_cols].max(axis=1) / (df_features['interest_total_score'] + 1e-8)\n        \n        # Top interest identification\n        df_features['top_interest_value'] = df_features[interest_cols].max(axis=1)\n        df_features['top_interest_category'] = df_features[interest_cols].idxmax(axis=1)\n        \n        # Interest gap features (user vs category median)\n        if 'f374' in df_features.columns:\n            for i in range(1, 13):\n                col = f'f{i}'\n                if col in df_features.columns:\n                    category_median = df_features.groupby('f374')[col].transform('median')\n                    df_features[f'interest_gap_f{i}'] = df_features[col] - category_median\n        \n        # Cross-feature interactions\n        key_interest_cols = ['f1', 'f2', 'f9', 'f11'] # automotive, dining, travel, electronics\n        available_key_interests = [col for col in key_interest_cols if col in df_features.columns]\n        \n        if available_key_interests and 'f376' in df_features.columns:\n            df_features['avg_key_interest'] = df_features[available_key_interests].mean(axis=1)\n            df_features['discount_interest_ratio'] = df_features['f376'] / (df_features['avg_key_interest'] + 1e-3)\n    \n    return df_features\n\n# Apply enhanced interest-offer alignment\nprint(\"Creating enhanced interest-offer alignment features...\")\ntrain_df_features = create_enhanced_interest_offer_alignment(train_df_features)\ntest_df_features = create_enhanced_interest_offer_alignment(test_df_features)\nprint(f\"Train shape after interest features: {train_df_features.shape}\")\nprint(f\"Test shape after interest features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:41:51.869094Z","iopub.execute_input":"2025-07-20T14:41:51.869369Z","iopub.status.idle":"2025-07-20T14:41:59.894378Z","shell.execute_reply.started":"2025-07-20T14:41:51.869348Z","shell.execute_reply":"2025-07-20T14:41:59.893373Z"}},"outputs":[{"name":"stdout","text":"Creating enhanced interest-offer alignment features...\nTrain shape after interest features: (770164, 479)\nTest shape after interest features: (369301, 478)\n","output_type":"stream"}],"execution_count":10},{"id":"8e7fd23d","cell_type":"code","source":"# Cell 10: Enhanced Loyalty Features\ndef create_enhanced_loyalty_features(df):\n    \"\"\"Enhanced loyalty features with cross-feature interactions\"\"\"\n    df_features = df.copy()\n    \n    # Membership level numerical encoding\n    membership_hierarchy = {\n        'Basic': 1, 'Silver': 2, 'Gold': 3, 'Platinum': 4, 'Diamond': 5\n    }\n    \n    if 'f42' in df_features.columns:\n        df_features['loyalty_tier_numerical'] = df_features['f42'].map(membership_hierarchy).fillna(0)\n        df_features['has_membership'] = (df_features['f42'].notna()).astype(int)\n    else:\n        df_features['loyalty_tier_numerical'] = 0\n        df_features['has_membership'] = 0\n    \n    # Miles and points features\n    if 'f43' in df_features.columns:\n        df_features['f43'] = pd.to_numeric(df_features['f43'], errors='coerce')\n        df_features['miles_balance_log'] = np.log1p(df_features['f43'].fillna(0))\n        df_features['has_miles_balance'] = (df_features['f43'] > 0).astype(int)\n    \n    if 'f47' in df_features.columns:\n        df_features['f47'] = pd.to_numeric(df_features['f47'], errors='coerce')\n        df_features['award_miles_log'] = np.log1p(df_features['f47'].fillna(0))\n        df_features['has_award_miles'] = (df_features['f47'] > 0).astype(int)\n    \n    # Miles utilization ratio\n    if 'f43' in df_features.columns and 'f47' in df_features.columns:\n        df_features['miles_utilization_ratio'] = df_features['f47'] / (df_features['f43'] + 1e-8)\n    \n    # Elite status features\n    if 'f46' in df_features.columns:\n        df_features['f46'] = pd.to_numeric(df_features['f46'], errors='coerce')\n        df_features['elite_qualifying_miles_log'] = np.log1p(df_features['f46'].fillna(0))\n        df_features['has_elite_activity'] = (df_features['f46'] > 0).astype(int)\n        df_features['elite_status_binary'] = (df_features['f46'] >= 25000).astype(int)\n        \n        # Elite tier numerical\n        elite_tier_temp = pd.cut(\n            df_features['f46'],\n            bins=[0, 25000, 50000, 75000, float('inf')],\n            labels=[0, 1, 2, 3]\n        )\n        df_features['elite_tier_numerical'] = elite_tier_temp.fillna(0).astype(int)\n    \n    # Customer value tier\n    if 'f53' in df_features.columns:\n        df_features['f53'] = pd.to_numeric(df_features['f53'], errors='coerce')\n        df_features['customer_value_tier'] = df_features['f53'].fillna(1)\n        df_features['is_high_value_customer'] = (df_features['f53'] >= 4).astype(int)\n        \n        # Value tier discount interaction\n        if 'f376' in df_features.columns:\n            discount_mean = df_features['f376'].mean()\n            discount_std = df_features['f376'].std()\n            df_features['discount_z_score'] = (df_features['f376'] - discount_mean) / (discount_std + 1e-8)\n            df_features['value_tier_discount'] = df_features['customer_value_tier'] * df_features['discount_z_score']\n    \n    # Loyalty program tenure\n    if 'f58' in df_features.columns:\n        df_features['f58'] = pd.to_numeric(df_features['f58'], errors='coerce')\n        df_features['loyalty_tenure_years'] = df_features['f58'] / 365.25\n        df_features['is_new_member'] = (df_features['f58'] <= 365).astype(int)\n        \n        # Tenure category as numerical\n        tenure_bins = pd.cut(df_features['f58'], bins=[0, 365, 1095, 1825, float('inf')], labels=[1, 2, 3, 4])\n        df_features['loyalty_tenure_category_num'] = tenure_bins.fillna(1).astype(int)\n    \n    # Composite loyalty engagement score\n    loyalty_components = []\n    for comp in ['loyalty_tier_numerical', 'has_miles_balance', 'has_elite_activity', 'customer_value_tier']:\n        if comp in df_features.columns:\n            loyalty_components.append(comp)\n    \n    if loyalty_components:\n        # Normalize components\n        normalized_components = []\n        for component in loyalty_components:\n            col_min = df_features[component].min()\n            col_max = df_features[component].max()\n            if col_max > col_min:\n                normalized = (df_features[component] - col_min) / (col_max - col_min)\n                normalized_components.append(normalized)\n        \n        if normalized_components:\n            df_features['loyalty_engagement_score'] = pd.concat(normalized_components, axis=1).mean(axis=1)\n    \n    return df_features\n\n# Apply enhanced loyalty features\nprint(\"Creating enhanced loyalty features...\")\ntrain_df_features = create_enhanced_loyalty_features(train_df_features)\ntest_df_features = create_enhanced_loyalty_features(test_df_features)\nprint(f\"Train shape after loyalty features: {train_df_features.shape}\")\nprint(f\"Test shape after loyalty features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:41:59.895426Z","iopub.execute_input":"2025-07-20T14:41:59.895766Z","iopub.status.idle":"2025-07-20T14:42:05.626194Z","shell.execute_reply.started":"2025-07-20T14:41:59.895741Z","shell.execute_reply":"2025-07-20T14:42:05.625291Z"}},"outputs":[{"name":"stdout","text":"Creating enhanced loyalty features...\nTrain shape after loyalty features: (770164, 498)\nTest shape after loyalty features: (369301, 497)\n","output_type":"stream"}],"execution_count":11},{"id":"88fabad2","cell_type":"code","source":"# Cell 11: Enhanced Offer Competition Features\ndef create_offer_competition_features(df):\n    \"\"\"Create offer competition and context features\"\"\"\n    df_features = df.copy()\n    \n    # Ensure we have required columns\n    if 'f374' not in df_features.columns or 'impression_time' not in df_features.columns:\n        print(\"Warning: Required columns missing for offer competition features\")\n        return df_features\n    \n    # Daily offer competition in same category\n    df_features['offers_in_same_category'] = df_features.groupby([\n        'id2', 'f374', \n        pd.Grouper(key='impression_time', freq='D')\n    ])['id3'].transform('count')\n    \n    # Offer density features\n    df_features['daily_offer_density'] = df_features.groupby([\n        'id2', \n        pd.Grouper(key='impression_time', freq='D')\n    ])['id3'].transform('count')\n    \n    # Category competition level\n    df_features['category_competition_level'] = pd.cut(\n        df_features['offers_in_same_category'],\n        bins=[0, 1, 3, 5, float('inf')],\n        labels=[1, 2, 3, 4]\n    ).fillna(1).astype(int)\n    \n    # Offer uniqueness score\n    df_features['offer_uniqueness'] = 1 / (df_features['offers_in_same_category'] + 1)\n    \n    # Time-based offer frequency\n    df_features['hourly_offer_frequency'] = df_features.groupby([\n        'id2', \n        pd.Grouper(key='impression_time', freq='H')\n    ])['id3'].transform('count')\n    \n    return df_features\n\n# Apply offer competition features\nprint(\"Creating offer competition features...\")\ntrain_df_features = create_offer_competition_features(train_df_features)\ntest_df_features = create_offer_competition_features(test_df_features)\nprint(f\"Train shape after competition features: {train_df_features.shape}\")\nprint(f\"Test shape after competition features: {test_df_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:05.627239Z","iopub.execute_input":"2025-07-20T14:42:05.627546Z","iopub.status.idle":"2025-07-20T14:42:38.699678Z","shell.execute_reply.started":"2025-07-20T14:42:05.627521Z","shell.execute_reply":"2025-07-20T14:42:38.698786Z"}},"outputs":[{"name":"stdout","text":"Creating offer competition features...\nTrain shape after competition features: (770164, 503)\nTest shape after competition features: (369301, 502)\n","output_type":"stream"}],"execution_count":12},{"id":"7885a3cb","cell_type":"markdown","source":"## Phase 3 - Time-Based Data Splitting and Feature Selection","metadata":{}},{"id":"9af04607","cell_type":"code","source":"# Cell 12: FIXED Time-Based Data Splitting Function\ndef create_time_based_split(df, time_threshold=None, train_ratio=0.8, method='quantile'):\n    \"\"\"Create time-based train/validation split to prevent data leakage\"\"\"\n    \n    if 'impression_time' not in df.columns:\n        print(\"Warning: impression_time column not found\")\n        return df, None\n    \n    # Calculate threshold dynamically if not provided\n    if time_threshold is None:\n        if method == 'quantile':\n            time_threshold = df['impression_time'].quantile(train_ratio)\n        elif method == 'sample_based':\n            sorted_dates = df['impression_time'].sort_values()\n            split_index = int(len(sorted_dates) * train_ratio)\n            time_threshold = sorted_dates.iloc[split_index]\n        elif method == 'daily_boundary':\n            daily_counts = df.groupby(df['impression_time'].dt.date).size()\n            daily_counts_cumsum = daily_counts.cumsum()\n            target_train_samples = int(len(df) * train_ratio)\n            split_day = daily_counts_cumsum[daily_counts_cumsum >= target_train_samples].index[0]\n            time_threshold = pd.Timestamp(split_day)\n        else:\n            # Default to quantile method\n            time_threshold = df['impression_time'].quantile(train_ratio)\n        \n        print(f\"Auto-calculated threshold using {method} method: {time_threshold}\")\n    else:\n        time_threshold = pd.Timestamp(time_threshold)\n        print(f\"Using provided threshold: {time_threshold}\")\n    \n    # Split data\n    train_temporal = df[df['impression_time'] < time_threshold].copy()\n    val_temporal = df[df['impression_time'] >= time_threshold].copy()\n    \n    # Validation check\n    if len(val_temporal) > 0:\n        # Ensure no data leakage\n        if len(train_temporal) > 0:\n            assert val_temporal['impression_time'].min() >= train_temporal['impression_time'].max(), \\\n                \"Data leakage detected: validation data overlaps with training data\"\n        \n        print(f\"Time-based split created successfully:\")\n        print(f\" Training: {len(train_temporal)} samples ({len(train_temporal)/len(df):.1%})\")\n        print(f\" Validation: {len(val_temporal)} samples ({len(val_temporal)/len(df):.1%})\")\n        print(f\" Training time range: {train_temporal['impression_time'].min()} to {train_temporal['impression_time'].max()}\")\n        print(f\" Validation time range: {val_temporal['impression_time'].min()} to {val_temporal['impression_time'].max()}\")\n        \n        return train_temporal, val_temporal\n    else:\n        print(\"Warning: No validation data found after time threshold\")\n        print(f\"Threshold: {time_threshold}\")\n        print(f\"Data range: {df['impression_time'].min()} to {df['impression_time'].max()}\")\n        return train_temporal, None\n\n# Test the function with your data\nprint(\"Testing time-based split function...\")\ntest_train, test_val = create_time_based_split(train_df_features, method='quantile', train_ratio=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:38.700801Z","iopub.execute_input":"2025-07-20T14:42:38.701161Z","iopub.status.idle":"2025-07-20T14:42:43.105276Z","shell.execute_reply.started":"2025-07-20T14:42:38.701112Z","shell.execute_reply":"2025-07-20T14:42:43.104365Z"}},"outputs":[{"name":"stdout","text":"Testing time-based split function...\nAuto-calculated threshold using quantile method: 2023-11-03 08:30:56.586400\nTime-based split created successfully:\n Training: 616131 samples (80.0%)\n Validation: 154033 samples (20.0%)\n Training time range: 2023-11-01 00:00:23.729000 to 2023-11-03 08:30:56.584000\n Validation time range: 2023-11-03 08:30:56.590000 to 2023-11-03 23:59:58.572000\n","output_type":"stream"}],"execution_count":13},{"id":"f61a9404","cell_type":"code","source":"# Cell 13: Feature Importance-Based Selection\ndef feature_importance_selection(train_df, target_col='y', n_features=400):\n    \"\"\"Select features based on LightGBM feature importance\"\"\"\n    \n    if target_col not in train_df.columns:\n        print(f\"Warning: Target column '{target_col}' not found\")\n        return train_df.columns.tolist()\n    \n    # Prepare features and target\n    feature_cols = [col for col in train_df.columns \n                   if not col.startswith('id') and col != target_col \n                   and col not in ['impression_time', 'click_time']]\n    \n    X = train_df[feature_cols].fillna(0) # Fill NaN for LightGBM\n    y = train_df[target_col]\n    \n    print(f\"Training LightGBM for feature selection on {len(feature_cols)} features...\")\n    \n    try:\n        # Train LightGBM model\n        model = LGBMClassifier(\n            n_estimators=100,\n            random_state=42,\n            verbose=-1\n        )\n        model.fit(X, y)\n        \n        # Get feature importance\n        importance = pd.Series(model.feature_importances_, index=feature_cols)\n        \n        # Select top N features\n        top_features = importance.nlargest(n_features).index.tolist()\n        \n        print(f\"Selected top {len(top_features)} features based on importance\")\n        print(f\"Top 10 most important features:\")\n        for i, (feat, imp) in enumerate(importance.nlargest(10).items()):\n            print(f\" {i+1}. {feat}: {imp:.4f}\")\n        \n        # Add back ID columns and target\n        id_cols = [col for col in train_df.columns if col.startswith('id')]\n        final_features = id_cols + top_features + [target_col]\n        \n        return final_features\n    \n    except Exception as e:\n        print(f\"Error in feature selection: {e}\")\n        return train_df.columns.tolist()\n\nprint(\"Feature importance selection function defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:43.106259Z","iopub.execute_input":"2025-07-20T14:42:43.106495Z","iopub.status.idle":"2025-07-20T14:42:43.116274Z","shell.execute_reply.started":"2025-07-20T14:42:43.106478Z","shell.execute_reply":"2025-07-20T14:42:43.115296Z"}},"outputs":[{"name":"stdout","text":"Feature importance selection function defined\n","output_type":"stream"}],"execution_count":14},{"id":"dbeba162","cell_type":"markdown","source":"## Phase 4 - Final Processing and Cleanup","metadata":{}},{"id":"fc081ee4","cell_type":"code","source":"# Cell 14: Comprehensive Feature Cleanup\ndef comprehensive_feature_cleanup(train_df, test_df):\n    \"\"\"Comprehensive cleanup of engineered features\"\"\"\n    \n    # Remove datetime columns that cause issues\n    datetime_columns_to_remove = [\n        'time_of_day'\n    ]\n    \n    # Remove categorical columns (keep numerical encodings)\n    categorical_columns_to_remove = [\n        'f374_top', 'f374', 'page_type', 'top_interest_category'\n    ]\n    \n    # Combine all columns to remove\n    columns_to_remove = datetime_columns_to_remove + categorical_columns_to_remove\n    \n    # Clean train dataset\n    train_cleaned = train_df.copy()\n    for col in columns_to_remove:\n        if col in train_cleaned.columns:\n            train_cleaned = train_cleaned.drop(columns=[col])\n    \n    # Clean test dataset\n    test_cleaned = test_df.copy()\n    for col in columns_to_remove:\n        if col in test_cleaned.columns:\n            test_cleaned = test_cleaned.drop(columns=[col])\n    \n    # Ensure column consistency\n    train_cols = set(train_cleaned.columns)\n    test_cols = set(test_cleaned.columns)\n    \n    common_cols = train_cols.intersection(test_cols)\n    train_only_cols = train_cols - test_cols\n    test_only_cols = test_cols - train_cols\n    \n    # Add missing columns with zeros\n    for col in train_only_cols:\n        if col != 'y': # Don't add target to test\n            test_cleaned[col] = 0\n    \n    for col in test_only_cols:\n        train_cleaned[col] = 0\n    \n    print(f\"Cleanup completed:\")\n    print(f\" Train shape: {train_cleaned.shape}\")\n    print(f\" Test shape: {test_cleaned.shape}\")\n    print(f\" Columns removed: {len(columns_to_remove)}\")\n    \n    return train_cleaned, test_cleaned\n\n# Apply comprehensive cleanup\nprint(\"Applying comprehensive feature cleanup...\")\ntrain_df_final, test_df_final = comprehensive_feature_cleanup(train_df_features, test_df_features)\nprint(\"Cleanup completed successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:43.117322Z","iopub.execute_input":"2025-07-20T14:42:43.117723Z","iopub.status.idle":"2025-07-20T14:42:54.827338Z","shell.execute_reply.started":"2025-07-20T14:42:43.117694Z","shell.execute_reply":"2025-07-20T14:42:54.826450Z"}},"outputs":[{"name":"stdout","text":"Applying comprehensive feature cleanup...\nCleanup completed:\n Train shape: (770164, 498)\n Test shape: (369301, 497)\n Columns removed: 5\nCleanup completed successfully\n","output_type":"stream"}],"execution_count":15},{"id":"a3544e97","cell_type":"code","source":"# # Cell 15: FIXED Apply Feature Selection and Final Processing\n\n# def finalize_feature_engineering(train_df, test_df):\n#     \"\"\"Apply feature selection and final processing with proper categorical handling and temporal column preservation\"\"\"\n    \n#     # STEP 1: Handle categorical columns before feature selection\n#     train_processed = train_df.copy()\n#     test_processed = test_df.copy()\n    \n#     # Identify categorical columns (object dtype)\n#     categorical_cols = train_processed.select_dtypes(include=['object']).columns.tolist()\n    \n#     # Remove target column from categorical processing if present\n#     if 'y' in categorical_cols:\n#         categorical_cols.remove('y')\n    \n#     # Remove temporal columns from categorical processing\n#     temporal_cols = ['impression_time', 'click_time']\n#     for col in temporal_cols:\n#         if col in categorical_cols:\n#             categorical_cols.remove(col)\n    \n#     # **CRITICAL FIX: Remove ID columns from categorical processing**\n#     id_columns = [col for col in categorical_cols if col.startswith('id')]\n#     for col in id_columns:\n#         if col in categorical_cols:\n#             categorical_cols.remove(col)\n    \n#     print(f\"Found {len(categorical_cols)} categorical columns to process: {categorical_cols}\")\n#     print(f\"Excluding ID columns from categorical processing: {id_columns}\")\n    \n#     # Process categorical columns (now excluding ID columns)\n#     for col in categorical_cols:\n#         if col in train_processed.columns:\n#             # Method 1: Label Encoding for ordinal categories\n#             if col in ['f42', 'f54', 'f55']: # Membership/status columns\n#                 # Create combined categories for consistent encoding\n#                 combined_values = pd.concat([\n#                     train_processed[col].dropna(), \n#                     test_processed[col].dropna() if col in test_processed.columns else pd.Series()\n#                 ]).unique()\n                \n#                 # Create label encoder\n#                 le = LabelEncoder()\n#                 le.fit(combined_values)\n                \n#                 # Apply encoding\n#                 train_processed[col] = train_processed[col].fillna('Unknown')\n#                 train_processed[col] = le.transform(train_processed[col])\n                \n#                 if col in test_processed.columns:\n#                     test_processed[col] = test_processed[col].fillna('Unknown')\n#                     # Handle unseen categories\n#                     test_processed[col] = test_processed[col].apply(\n#                         lambda x: le.transform([x])[0] if x in le.classes_ else -1\n#                     )\n#             # Method 2: Frequency encoding for high-cardinality categories\n#             else:\n#                 # Calculate frequency encoding from training data\n#                 freq_encoding = train_processed[col].value_counts().to_dict()\n                \n#                 # Apply frequency encoding\n#                 train_processed[col] = train_processed[col].map(freq_encoding).fillna(0)\n                \n#                 if col in test_processed.columns:\n#                     test_processed[col] = test_processed[col].map(freq_encoding).fillna(0)\n    \n#     # STEP 2: Clean feature names for LightGBM compatibility\n#     def clean_feature_names_for_lightgbm(df):\n#         \"\"\"Clean column names to remove special JSON characters\"\"\"\n#         import re\n#         new_names = {}\n#         seen = set()\n        \n#         for col in df.columns:\n#             # Remove all characters except letters, numbers, and underscore\n#             clean_col = re.sub(r'[^A-Za-z0-9_]+', '', col)\n            \n#             # Handle empty names\n#             if not clean_col:\n#                 clean_col = 'feature'\n            \n#             # Ensure unique column names\n#             if clean_col in seen:\n#                 i = 1\n#                 new_col = f\"{clean_col}_{i}\"\n#                 while new_col in seen:\n#                     i += 1\n#                     new_col = f\"{clean_col}_{i}\"\n#                 clean_col = new_col\n            \n#             seen.add(clean_col)\n#             new_names[col] = clean_col\n        \n#         return df.rename(columns=new_names)\n    \n#     # Apply feature name cleaning\n#     print(\"Cleaning feature names for LightGBM compatibility...\")\n#     train_processed = clean_feature_names_for_lightgbm(train_processed)\n#     test_processed = clean_feature_names_for_lightgbm(test_processed)\n    \n#     # STEP 3: Preserve temporal columns before feature selection\n#     temporal_cols = ['impression_time', 'click_time']\n#     preserved_temporal = {}\n    \n#     for col in temporal_cols:\n#         if col in train_processed.columns:\n#             preserved_temporal[f'train_{col}'] = train_processed[col].copy()\n#         if col in test_processed.columns:\n#             preserved_temporal[f'test_{col}'] = test_processed[col].copy()\n    \n#     # STEP 4: Prepare features for selection (exclude temporal columns and ID columns)\n#     feature_cols = [col for col in train_processed.columns \n#                     if not col.startswith('id') and col != 'y' \n#                     and col not in temporal_cols]\n    \n#     # Convert any remaining non-numeric columns\n#     for col in feature_cols:\n#         if col in train_processed.columns:\n#             train_processed[col] = pd.to_numeric(train_processed[col], errors='coerce').fillna(0)\n#         if col in test_processed.columns:\n#             test_processed[col] = pd.to_numeric(test_processed[col], errors='coerce').fillna(0)\n    \n#     print(f\"All categorical columns processed and feature names cleaned. Ready for feature selection.\")\n    \n#     # STEP 5: Apply feature importance selection (excluding temporal columns)\n#     selected_features = feature_importance_selection(train_processed, target_col='y', n_features=400)\n    \n#     # STEP 6: Add back temporal columns to final selection\n#     final_selected_features = selected_features.copy()\n    \n#     # Add temporal columns back if they exist\n#     for col in temporal_cols:\n#         if col in train_processed.columns and col not in final_selected_features:\n#             final_selected_features.append(col)\n    \n#     print(f\"Preserved temporal columns: {[col for col in temporal_cols if col in train_processed.columns]}\")\n    \n#     # Apply selection to both datasets\n#     train_selected = train_processed[final_selected_features].copy()\n#     test_selected_features = [col for col in final_selected_features if col in test_processed.columns]\n#     test_selected = test_processed[test_selected_features].copy()\n    \n#     # STEP 7: Final imputation for critical features\n#     critical_features = [\n#         'user_rolling_ctr_7d', 'user_rolling_ctr_3d', 'page_historical_ctr',\n#         'user_page_historical_ctr', 'interest_total_score', 'loyalty_engagement_score'\n#     ]\n    \n#     for feature in critical_features:\n#         if feature in train_selected.columns:\n#             train_selected[feature] = train_selected[feature].fillna(0)\n#         if feature in test_selected.columns:\n#             test_selected[feature] = test_selected[feature].fillna(0)\n    \n#     print(f\"Final feature engineering completed:\")\n#     print(f\" Final train shape: {train_selected.shape}\")\n#     print(f\" Final test shape: {test_selected.shape}\")\n#     print(f\" Features selected (excluding temporal): {len([f for f in final_selected_features if f not in temporal_cols and f != 'y'])}\")\n#     print(f\" Temporal columns preserved: {[col for col in temporal_cols if col in train_selected.columns]}\")\n    \n#     return train_selected, test_selected\n\n\n# # Apply final feature engineering\n# print(\"Applying final feature engineering...\")\n# train_final, test_final = finalize_feature_engineering(train_df_final, test_df_final)\n# print(\"Final processing completed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:54.828427Z","iopub.execute_input":"2025-07-20T14:42:54.829103Z","iopub.status.idle":"2025-07-20T14:42:54.836585Z","shell.execute_reply.started":"2025-07-20T14:42:54.829078Z","shell.execute_reply":"2025-07-20T14:42:54.835804Z"}},"outputs":[],"execution_count":16},{"id":"c51faa89-f6e7-4f19-8a6d-b3837bc44c12","cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# import gc\n\n# def finalize_feature_engineering(train_df, test_df):\n#     \"\"\"Optimized: Feature engineering with minimal RAM and no data quality loss\"\"\"\n\n#     # --- STEP 1: Identify column roles early ---\n#     temporal_cols = ['impression_time', 'click_time']\n#     target_col = 'y'\n#     id_cols = [c for c in train_df.columns if c.startswith('id')]\n    \n#     # Categoricals (excluding target, temporal, id columns)\n#     categorical_cols = [\n#         c for c in train_df.select_dtypes(include='object').columns \n#         if c not in temporal_cols + [target_col] and not c.startswith('id')\n#     ]\n#     print(f\"Categoricals: {categorical_cols}\")\n\n#     # --- STEP 2: Column selection (early filtering) ---\n#     feature_cols = [\n#         c for c in train_df.columns \n#         if c not in id_cols + temporal_cols + [target_col]\n#     ]\n#     # Add back temporal for final output after selection\n\n#     # --- STEP 3: Downcast numerics, set categoricals dtype ---\n#     for col in feature_cols:\n#         if train_df[col].dtype in ['int64', 'float64']:\n#             train_df[col] = pd.to_numeric(train_df[col], downcast='float')\n#             if col in test_df:\n#                 test_df[col] = pd.to_numeric(test_df[col], downcast='float')\n#     for col in categorical_cols:\n#         train_df[col] = train_df[col].astype('category')\n#         if col in test_df.columns:\n#             test_df[col] = test_df[col].astype('category')\n\n#     gc.collect()\n\n#     # --- STEP 4: Efficient encoding for categoricals ---\n#     for col in categorical_cols:\n#         if col in ['f42', 'f54', 'f55']:   # likely ordinal\n#             le = LabelEncoder()\n#             # Combine categories found in train and test for symmetry\n#             all_vals = pd.concat([\n#                 train_df[col], test_df[col] if col in test_df else pd.Series([], dtype=\"category\")\n#             ]).astype(str)\n#             le.fit(all_vals)\n#             train_df[col] = le.transform(train_df[col].astype(str))\n#             if col in test_df:\n#                 # Map test values; unknown to -1\n#                 valid_map = {cat: idx for idx, cat in enumerate(le.classes_)}\n#                 test_df[col] = test_df[col].astype(str).map(valid_map).fillna(-1).astype(int)\n#         else:\n#             # Frequency encode (fast, vectorized)\n#             freqs = train_df[col].value_counts(dropna=False)\n#             train_df[col] = train_df[col].map(freqs).astype(np.float32)\n#             if col in test_df:\n#                 test_df[col] = test_df[col].map(freqs).fillna(0).astype(np.float32)\n#     gc.collect()\n\n#     # --- STEP 5: Clean column names for LightGBM ---\n#     import re\n#     def clean_names(df):\n#         renamer = {}\n#         existing = set()\n#         for c in df.columns:\n#             name = re.sub(r'[^A-Za-z0-9_]+', '', c)\n#             if not name: name = 'feature'\n#             if name in existing:\n#                 base = name\n#                 idx = 1\n#                 while f\"{base}_{idx}\" in existing: idx += 1\n#                 name = f\"{base}_{idx}\"\n#             existing.add(name)\n#             renamer[c] = name\n#         return df.rename(columns=renamer)\n#     train_df = clean_names(train_df)\n#     test_df  = clean_names(test_df)\n\n#     # --- STEP 6: Feature selection (excluding temporals & id) ---\n#     # Only pass necessary columns & target to selector\n#     train_features = [c for c in train_df.columns if c not in id_cols + temporal_cols + [target_col]]\n#     # <--- CALL YOUR SMART SELECTION FUNCTION (must exist in scope) --->\n#     selected_features = feature_importance_selection(\n#         train_df[train_features + [target_col]],\n#         target_col=target_col, \n#         n_features=400\n#     )\n#     # Add back temporal columns if present\n#     for col in temporal_cols:\n#         if col in train_df.columns and col not in selected_features:\n#             selected_features.append(col)\n\n#     print(f\"Selected features: {len(selected_features)}\")\n\n#     # --- STEP 7: Assemble & impute final outputs ---\n#     train_final = train_df[selected_features + [target_col]].copy()\n#     test_final  = test_df[[c for c in selected_features if c in test_df]].copy()\n\n#     # Final imputation for critical features\n#     critical_features = [\n#         'user_rolling_ctr_7d', 'user_rolling_ctr_3d', 'page_historical_ctr',\n#         'user_page_historical_ctr', 'interest_total_score', 'loyalty_engagement_score'\n#     ]\n#     for col in critical_features:\n#         if col in train_final:\n#             train_final[col] = train_final[col].fillna(0)\n#         if col in test_final:\n#             test_final[col] = test_final[col].fillna(0)\n#     gc.collect()\n\n#     print(f\"Final train shape: {train_final.shape}\")\n#     print(f\"Final test shape:  {test_final.shape}\")\n#     print(f\"Preserved temporals: {[c for c in temporal_cols if c in train_final]}\")\n#     return train_final, test_final\n\n# # Run finalized feature engineering\n# print(\"Applying optimized, memory-efficient feature engineering...\")\n# train_final, test_final = finalize_feature_engineering(train_df_final, test_df_final)\n# print(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:54.837522Z","iopub.execute_input":"2025-07-20T14:42:54.837836Z","iopub.status.idle":"2025-07-20T14:42:54.862549Z","shell.execute_reply.started":"2025-07-20T14:42:54.837814Z","shell.execute_reply":"2025-07-20T14:42:54.861764Z"}},"outputs":[],"execution_count":17},{"id":"4eb870ac-af89-416d-b101-a4320029775a","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport re\n\ndef finalize_feature_engineering(train_df, test_df):\n    \"\"\"Optimized, safe feature engineering: memory friendly, with IDs preserved for validation/export.\"\"\"\n\n    # ----------- IDENTIFY ROLES ---------------\n    target_col = 'y'\n    temporal_cols = ['impression_time', 'click_time']\n    id_cols = [c for c in train_df.columns if c.startswith('id')]\n    \n    # Categorical (non-ID, non-temporal, non-target)\n    categorical_cols = [\n        c for c in train_df.select_dtypes(include='object').columns \n        if c not in id_cols + temporal_cols + [target_col]\n    ]\n    print(f\"[FE] Categorical columns: {categorical_cols}\")\n    \n    # --------- EARLY COLUMN FILTERING ----------\n    feature_cols = [\n        c for c in train_df.columns \n        if c not in id_cols + temporal_cols + [target_col]\n    ]\n\n    # ------------ DOWNSCALING TYPES --------------\n    for col in feature_cols:\n        if train_df[col].dtype in ('int64', 'float64'):\n            train_df[col] = pd.to_numeric(train_df[col], downcast='float')\n            if col in test_df:\n                test_df[col] = pd.to_numeric(test_df[col], downcast='float')\n    for col in categorical_cols:\n        # Use pandas categorical to save immense RAM\n        train_df[col] = train_df[col].astype('category')\n        if col in test_df.columns:\n            test_df[col] = test_df[col].astype('category')\n    gc.collect()\n\n    # ------------ ENCODING CATEGORICALS -------------\n    for col in categorical_cols:\n        if col in ['f42', 'f54', 'f55']:\n            # Label encode with classes from both train and test\n            le = LabelEncoder()\n            all_vals = pd.concat([\n                train_df[col], test_df[col] if col in test_df.columns else pd.Series([], dtype=\"category\")\n            ]).astype(str)\n            le.fit(all_vals)\n            train_df[col] = le.transform(train_df[col].astype(str))\n            if col in test_df:\n                valid_map = {cat: idx for idx, cat in enumerate(le.classes_)}\n                test_df[col] = test_df[col].astype(str).map(valid_map).fillna(-1).astype(int)\n        else:\n            # Frequency encode, vectorized\n            freqs = train_df[col].value_counts(dropna=False)\n            train_df[col] = train_df[col].map(freqs).astype(np.float32)\n            if col in test_df.columns:\n                test_df[col] = test_df[col].map(freqs).fillna(0).astype(np.float32)\n    gc.collect()\n\n    # ------------ LIGHTGBM-FRIENDLY COLUMN NAMES ------------\n    def clean_names(df):\n        renamer = {}\n        existing = set()\n        for c in df.columns:\n            name = re.sub(r'[^A-Za-z0-9_]+', '', c)\n            if not name: name = 'feature'\n            if name in existing:\n                base = name\n                idx = 1\n                while f\"{base}_{idx}\" in existing: idx += 1\n                name = f\"{base}_{idx}\"\n            existing.add(name)\n            renamer[c] = name\n        return df.rename(columns=renamer)\n    train_df = clean_names(train_df)\n    test_df  = clean_names(test_df)\n    gc.collect()\n\n    # ------------ FEATURE SELECTION -----------------\n    train_features = [\n        c for c in train_df.columns\n        if c not in id_cols + temporal_cols + [target_col]\n    ]\n    # You must provide your feature_importance_selection\n    selected_features = feature_importance_selection(\n        train_df[train_features + [target_col]],\n        target_col=target_col,\n        n_features=400\n    )\n    # Add back temporals for final DataFrame\n    for col in temporal_cols:\n        if col in train_df.columns and col not in selected_features:\n            selected_features.append(col)\n\n    # ------------ FINAL DF ASSEMBLY ----------------\n    # Use .copy() ONCE to avoid SettingWithCopy/side-effects\n    train_final = train_df[selected_features + [target_col]].copy()\n    test_final  = test_df[[c for c in selected_features if c in test_df.columns]].copy()\n\n    # ------------ CRITICAL IMPUTATION --------------\n    critical_features = [\n        'user_rolling_ctr_7d', 'user_rolling_ctr_3d', 'page_historical_ctr',\n        'user_page_historical_ctr', 'interest_total_score', 'loyalty_engagement_score'\n    ]\n    for col in critical_features:\n        if col in train_final:\n            train_final[col] = train_final[col].fillna(0)\n        if col in test_final:\n            test_final[col] = test_final[col].fillna(0)\n    gc.collect()\n\n    # ------------ PRESERVE ID COLUMNS (validation/output only!) ----------------\n    # (Preserve original order, don't use these as features for model input)\n    for col in id_cols:\n        if col not in train_final and col in train_df:\n            train_final[col] = train_df[col].values\n        if col not in test_final and col in test_df:\n            test_final[col] = test_df[col].values\n\n    print(f\"[FE] Final train shape: {train_final.shape}\")\n    print(f\"[FE] Final test shape:  {test_final.shape}\")\n    print(f\"[FE] Preserved temporals: {[c for c in temporal_cols if c in train_final]}  |  IDs: {id_cols}\")\n    return train_final, test_final\n\n# For usage:\nprint(\"Applying optimized feature engineering (with ID columns preserved for validation/export only)...\")\ntrain_final, test_final = finalize_feature_engineering(train_df_final, test_df_final)\nprint(\"Done.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:42:54.863693Z","iopub.execute_input":"2025-07-20T14:42:54.864201Z","iopub.status.idle":"2025-07-20T14:44:23.089799Z","shell.execute_reply.started":"2025-07-20T14:42:54.864170Z","shell.execute_reply":"2025-07-20T14:44:23.088844Z"}},"outputs":[{"name":"stdout","text":"Applying optimized feature engineering (with ID columns preserved for validation/export only)...\n[FE] Categorical columns: ['f42', 'f50', 'f52', 'f54', 'f55', 'f56', 'f57', 'f354', 'f378']\nTraining LightGBM for feature selection on 484 features...\nSelected top 400 features based on importance\nTop 10 most important features:\n 1. time_since_last_impression: 550.0000\n 2. user_session_length: 85.0000\n 3. f366: 68.0000\n 4. hourly_offer_frequency: 58.0000\n 5. f223: 47.0000\n 6. f132: 46.0000\n 7. f207: 46.0000\n 8. user_cumulative_impressions: 46.0000\n 9. f363: 45.0000\n 10. user_avg_session_length: 43.0000\n[FE] Final train shape: (770164, 415)\n[FE] Final test shape:  (369301, 413)\n[FE] Preserved temporals: ['impression_time', 'click_time']  |  IDs: ['id1', 'id2', 'id3', 'id4', 'id5', 'id10', 'id8', 'id12', 'id13', 'id6', 'id7']\nDone.\n","output_type":"stream"}],"execution_count":18},{"id":"5b75317b","cell_type":"code","source":"# New Cell: Data Integrity Validation\ndef validate_data_integrity(train_df, test_df):\n    \"\"\"Validate that ID columns and datetime columns haven't been corrupted\"\"\"\n    \n    print(\"=== DATA INTEGRITY VALIDATION ===\")\n    \n    # Check ID columns\n    id_columns = [col for col in train_df.columns if col.startswith('id')]\n    print(f\"ID columns found: {id_columns}\")\n    \n    for col in id_columns:\n        if col in train_df.columns:\n            # Check if ID values look reasonable (not frequency encoded)\n            unique_count = train_df[col].nunique()\n            total_count = len(train_df)\n            uniqueness_ratio = unique_count / total_count\n            \n            print(f\"  {col}: {unique_count} unique values, ratio: {uniqueness_ratio:.4f}\")\n            \n            # Flag if ID column looks corrupted (too low uniqueness for IDs)\n            if col in ['id1', 'id2', 'id3'] and uniqueness_ratio < 0.1:\n                print(f\"  WARNING: {col} may be corrupted (low uniqueness ratio)\")\n    \n    # Check datetime columns\n    datetime_cols = ['impression_time', 'click_time']\n    for col in datetime_cols:\n        if col in train_df.columns:\n            min_date = train_df[col].min()\n            max_date = train_df[col].max()\n            \n            print(f\"  {col}: {min_date} to {max_date}\")\n            \n            # Flag if dates reset to 1970\n            if pd.to_datetime(min_date).year < 2000:\n                print(f\"  ERROR: {col} contains dates before year 2000 - likely corrupted!\")\n            else:\n                print(f\"   {col} dates look valid\")\n    \n    print(\"=== VALIDATION COMPLETE ===\")\n\n# Run validation\nvalidate_data_integrity(train_final, test_final)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:44:23.090719Z","iopub.execute_input":"2025-07-20T14:44:23.091125Z","iopub.status.idle":"2025-07-20T14:44:23.982947Z","shell.execute_reply.started":"2025-07-20T14:44:23.091102Z","shell.execute_reply":"2025-07-20T14:44:23.981911Z"}},"outputs":[{"name":"stdout","text":"=== DATA INTEGRITY VALIDATION ===\nID columns found: ['id1', 'id2', 'id3', 'id4', 'id5', 'id10', 'id8', 'id12', 'id13', 'id6', 'id7']\n  id1: 770164 unique values, ratio: 1.0000\n  id2: 46550 unique values, ratio: 0.0604\n  WARNING: id2 may be corrupted (low uniqueness ratio)\n  id3: 757 unique values, ratio: 0.0010\n  WARNING: id3 may be corrupted (low uniqueness ratio)\n  id4: 763680 unique values, ratio: 0.9916\n  id5: 3 unique values, ratio: 0.0000\n  id10: 2 unique values, ratio: 0.0000\n  id8: 186 unique values, ratio: 0.0002\n  id12: 108 unique values, ratio: 0.0001\n  id13: 119 unique values, ratio: 0.0002\n  id6: 0 unique values, ratio: 0.0000\n  id7: 0 unique values, ratio: 0.0000\n  impression_time: 2023-11-01 00:00:23.729000 to 2023-11-03 23:59:58.572000\n   impression_time dates look valid\n  click_time: NaT to NaT\n   click_time dates look valid\n=== VALIDATION COMPLETE ===\n","output_type":"stream"}],"execution_count":19},{"id":"3c4fa8d6","cell_type":"code","source":"# Cell 16: FIXED Save Final Datasets with Dynamic Threshold\ndef save_final_datasets(train_df, test_df, optimal_threshold=None):\n    \"\"\"Save the final processed datasets with proper time-based validation split\"\"\"\n    \n    import os\n    os.makedirs('dataset/stage_final', exist_ok=True)\n    \n    # Save datasets\n    train_df.to_csv('dataset/stage_final/train_features_final.csv', index=False)\n    test_df.to_csv('dataset/stage_final/test_features_final.csv', index=False)\n    \n    # Save feature names\n    feature_names = [col for col in train_df.columns if col != 'y']\n    pd.DataFrame({'feature_name': feature_names}).to_csv('dataset/stage_final/final_feature_names.csv', index=False)\n    \n    print(f\"Final datasets saved:\")\n    print(f\" Train: dataset/stage_final/train_features_final.csv ({train_df.shape})\")\n    print(f\" Test: dataset/stage_final/test_features_final.csv ({test_df.shape})\")\n    print(f\" Features: dataset/stage_final/final_feature_names.csv ({len(feature_names)} features)\")\n    \n    # FIXED: Use dynamic threshold calculation\n    if optimal_threshold is None:\n        # Calculate threshold based on your data\n        optimal_threshold = train_df['impression_time'].quantile(0.8)\n        print(f\"Using auto-calculated threshold: {optimal_threshold}\")\n    else:\n        print(f\"Using provided threshold: {optimal_threshold}\")\n    \n    # Create time-based validation split with dynamic threshold\n    train_temporal, val_temporal = create_time_based_split(\n        train_df, \n        time_threshold=optimal_threshold,\n        method='quantile'\n    )\n    \n    if val_temporal is not None and len(val_temporal) > 0:\n        train_temporal.to_csv('dataset/stage_final/train_temporal.csv', index=False)\n        val_temporal.to_csv('dataset/stage_final/val_temporal.csv', index=False)\n        print(f\" Temporal split saved:\")\n        print(f\" Train temporal: dataset/stage_final/train_temporal.csv ({train_temporal.shape})\")\n        print(f\" Val temporal: dataset/stage_final/val_temporal.csv ({val_temporal.shape})\")\n    else:\n        print(\"Warning: Could not create validation split. Saving full training set only.\")\n        train_df.to_csv('dataset/stage_final/train_temporal.csv', index=False)\n\n# Save final datasets using the optimal threshold calculated earlier\nprint(\"Saving final datasets...\")\nsave_final_datasets(train_final, test_final, optimal_threshold=optimal_threshold)\n\nprint(\"\\n=== FEATURE ENGINEERING PIPELINE COMPLETED ===\")\nprint(\" Data leakage issues fixed\")\nprint(\" Enhanced temporal features added\")\nprint(\" Improved offer-user affinity features\")\nprint(\" Feature selection applied\")\nprint(\" Time-based validation split created with DYNAMIC threshold\")\nprint(\" All datasets saved successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:44:23.983961Z","iopub.execute_input":"2025-07-20T14:44:23.984297Z","iopub.status.idle":"2025-07-20T14:55:11.777912Z","shell.execute_reply.started":"2025-07-20T14:44:23.984265Z","shell.execute_reply":"2025-07-20T14:55:11.776821Z"}},"outputs":[{"name":"stdout","text":"Saving final datasets...\nFinal datasets saved:\n Train: dataset/stage_final/train_features_final.csv ((770164, 415))\n Test: dataset/stage_final/test_features_final.csv ((369301, 413))\n Features: dataset/stage_final/final_feature_names.csv (413 features)\nUsing provided threshold: 2023-11-03 08:30:56.586400\nUsing provided threshold: 2023-11-03 08:30:56.586400\nTime-based split created successfully:\n Training: 616131 samples (80.0%)\n Validation: 154033 samples (20.0%)\n Training time range: 2023-11-01 00:00:23.729000 to 2023-11-03 08:30:56.584000\n Validation time range: 2023-11-03 08:30:56.590000 to 2023-11-03 23:59:58.572000\n Temporal split saved:\n Train temporal: dataset/stage_final/train_temporal.csv ((616131, 415))\n Val temporal: dataset/stage_final/val_temporal.csv ((154033, 415))\n\n=== FEATURE ENGINEERING PIPELINE COMPLETED ===\n Data leakage issues fixed\n Enhanced temporal features added\n Improved offer-user affinity features\n Feature selection applied\n Time-based validation split created with DYNAMIC threshold\n All datasets saved successfully\n","output_type":"stream"}],"execution_count":20},{"id":"7cc613c8-0cb5-4dc8-9722-ba4ac2a78135","cell_type":"code","source":"print(set(train_final.columns) - set(test_final.columns))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:11.779373Z","iopub.execute_input":"2025-07-20T14:55:11.779699Z","iopub.status.idle":"2025-07-20T14:55:11.785522Z","shell.execute_reply.started":"2025-07-20T14:55:11.779665Z","shell.execute_reply":"2025-07-20T14:55:11.784512Z"}},"outputs":[{"name":"stdout","text":"{'y'}\n","output_type":"stream"}],"execution_count":21},{"id":"b3fd78cc-f91c-48d6-a4f9-4dc908f9eb21","cell_type":"code","source":"print(train_final.columns.sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:11.786585Z","iopub.execute_input":"2025-07-20T14:55:11.786948Z","iopub.status.idle":"2025-07-20T14:55:12.023394Z","shell.execute_reply.started":"2025-07-20T14:55:11.786926Z","shell.execute_reply":"2025-07-20T14:55:12.022351Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/522654797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'sum'"],"ename":"AttributeError","evalue":"'Index' object has no attribute 'sum'","output_type":"error"}],"execution_count":22},{"id":"b850c0f7-06fb-426d-9cbd-41552d785059","cell_type":"code","source":"from collections import Counter\n\n# For any DataFrame, e.g., train_final\ncol_counts = Counter(train_final.columns)\n# Only show columns that appear more than once\nfor col, freq in col_counts.items():\n    if freq > 1:\n        print(f\"Column: {col}   Frequency: {freq}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:12.024037Z","iopub.status.idle":"2025-07-20T14:55:12.024364Z","shell.execute_reply.started":"2025-07-20T14:55:12.024201Z","shell.execute_reply":"2025-07-20T14:55:12.024214Z"}},"outputs":[],"execution_count":null},{"id":"15f88399-dee6-4b55-8cf0-687f75eeed9e","cell_type":"code","source":"train_final['y']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:12.026027Z","iopub.status.idle":"2025-07-20T14:55:12.026328Z","shell.execute_reply.started":"2025-07-20T14:55:12.026198Z","shell.execute_reply":"2025-07-20T14:55:12.026211Z"}},"outputs":[],"execution_count":null},{"id":"0924b9e5-04c2-489f-9eaf-bfe2daf34036","cell_type":"code","source":"train_df['y']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:12.027320Z","iopub.status.idle":"2025-07-20T14:55:12.027569Z","shell.execute_reply.started":"2025-07-20T14:55:12.027449Z","shell.execute_reply":"2025-07-20T14:55:12.027461Z"}},"outputs":[],"execution_count":null},{"id":"d0d617be-9031-4872-b52d-c9c49961e2eb","cell_type":"code","source":"print(train_final['y'].nunique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:55:12.028679Z","iopub.status.idle":"2025-07-20T14:55:12.028998Z","shell.execute_reply.started":"2025-07-20T14:55:12.028816Z","shell.execute_reply":"2025-07-20T14:55:12.028831Z"}},"outputs":[],"execution_count":null}]}